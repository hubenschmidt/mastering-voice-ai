{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "751d7dc1-5f3a-4059-baaa-e1dbeb8edd57",
   "metadata": {},
   "source": [
    "# Traditional Voice Interaction Pipeline Implementation\n",
    "\n",
    "## Short Description\n",
    "This code demonstrates a sequential voice assistant pipeline with three decoupled modules: Automatic Speech Recognition (ASR), Large Language Model (LLM) processing, and Text-to-Speech (TTS) synthesis. It illustrates the modular architecture and sequential execution flow of traditional voice assistants, highlighting both benefits and limitations discussed in lecture.\n",
    "\n",
    "## Key Libraries Used\n",
    "- `speech_recognition`: For capturing audio and performing speech-to-text conversion\n",
    "- `transformers`: Provides access to the GPT-2 language model for text generation\n",
    "- `gtts`: Google Text-to-Speech API interface for speech synthesis\n",
    "- `playsound`: For audio playback of generated responses\n",
    "\n",
    "## Code Logic and Flow\n",
    "\n",
    "### High-Level Overview\n",
    "The script implements a strictly sequential pipeline where:\n",
    "1. Audio input is captured through the microphone and converted to text\n",
    "2. The transcribed text is processed by a language model to generate a response\n",
    "3. The text response is converted to speech and played through speakers\n",
    "The execution flows unidirectionally with no feedback mechanisms between stages.\n",
    "\n",
    "### Visual Flowchart\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start] --> B[Capture Audio]\n",
    "    B --> C{ASR Success?}\n",
    "    C -->|Yes| D[Process Text with LLM]\n",
    "    C -->|No| E[Generate Error Message]\n",
    "    D --> F[Convert Text to Speech]\n",
    "    E --> F\n",
    "    F --> G[Play Audio]\n",
    "    G --> H[Cleanup Resources]\n",
    "    H --> I[End]\n",
    "```\n",
    "## Step-by-Step Code Breakdown\n",
    "\n",
    "#### Step 1: Automatic Speech Recognition (ASR)\n",
    "\n",
    "    Captures live audio input through the microphone\n",
    "\n",
    "    Performs ambient noise reduction for better accuracy\n",
    "\n",
    "    Sends audio to Google's speech recognition API\n",
    "\n",
    "    Handles two primary error cases:\n",
    "\n",
    "        Unrecognizable audio (returns None)\n",
    "\n",
    "        Service connection errors (returns None)\n",
    "\n",
    "    Returns transcribed text or error indicator\n",
    "\n",
    "#### Step 2: Large Language Model (LLM) Processing\n",
    "\n",
    "    Validates ASR output before processing\n",
    "\n",
    "    Constructs conversational prompt incorporating user input\n",
    "\n",
    "    Uses GPT-2 model to generate contextual response\n",
    "\n",
    "    Extracts first relevant line from model output\n",
    "\n",
    "    Provides fallback message for invalid inputs\n",
    "\n",
    "    Demonstrates text-only processing (no audio context)\n",
    "\n",
    "#### Step 3: Text-to-Speech (TTS) Synthesis\n",
    "\n",
    "    Converts text response to spoken audio\n",
    "\n",
    "    Uses Google's TTS service for speech generation\n",
    "\n",
    "    Saves generated audio to temporary file\n",
    "\n",
    "    Plays audio through system speakers\n",
    "\n",
    "    Cleans up temporary files post-playback\n",
    "\n",
    "    Demonstrates output-only conversion\n",
    "\n",
    "#### Pipeline Execution\n",
    "\n",
    "    Coordinates strict sequential execution of stages\n",
    "\n",
    "    ASR must complete before LLM processing starts\n",
    "\n",
    "    LLM must finish before TTS begins\n",
    "\n",
    "    No error recovery between stages\n",
    "\n",
    "    Shows linear waterfall execution pattern\n",
    "\n",
    "### Connecting to the Lecture\n",
    "\n",
    "This implementation concretely demonstrates the traditional pipeline architecture discussed in lecture:\n",
    "\n",
    "    Modular Separation: Clear boundaries between ASR, LLM, and TTS components\n",
    "\n",
    "    Sequential Processing: Strict stage-by-stage execution creates latency accumulation\n",
    "\n",
    "    Error Propagation: ASR failures directly impact downstream components\n",
    "\n",
    "    Context Loss: Prosody and emotional cues are stripped during ASR conversion\n",
    "\n",
    "    Temporary Artifacts: File-based handling between components increases latency\n",
    "\n",
    "The code intentionally shows architectural constraints that modern end-to-end SpeechLMs address through integrated audio-to-audio processing, preserving paralinguistic features and enabling real-time feedback.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc9f2e7-a8ab-4009-8b03-d25ce4d4ef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting playsound==1.2.2\n",
      "  Downloading playsound-1.2.2-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Downloading playsound-1.2.2-py2.py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: playsound\n",
      "Successfully installed playsound-1.2.2\n",
      "Requirement already satisfied: openai in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: python-dotenv in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (4.67.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages - Use Anaconda shell to install them\n",
    "!pip install speechrecognition pyttsx3 openai gTTS --quiet\n",
    "!pip install playsound==1.2.2\n",
    "!pip install openai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e07731a-f1ba-4181-a4ca-162d2ceeb5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (3.14.5)\n",
      "Requirement already satisfied: typing-extensions in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from SpeechRecognition) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60afc861-f479-489c-8f3d-01a4687106a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: typer>=0.23.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from typer-slim->transformers) (0.23.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim->transformers) (8.1.8)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim->transformers) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from typer>=0.23.0->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->transformers) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4875f51-ff6c-4e86-8b91-1ef8fa956636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: filelock in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from torch) (80.10.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/hubenschmidt/anaconda3/envs/SpeechLM/lib/python3.12/site-packages (from torch) (2026.2.0)\n",
      "Collecting cuda-bindings==12.9.4 (from torch)\n",
      "  Downloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.4.5 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.6.0 (from torch)\n",
      "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting cuda-pathfinder~=1.1 (from cuda-bindings==12.9.4->torch)\n",
      "  Downloading cuda_pathfinder-1.3.4-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl (915.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m  \u001b[33m0:01:01\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Downloading nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (139.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_pathfinder-1.3.4-py3-none-any.whl (30 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, cuda-pathfinder, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, cuda-bindings, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/24\u001b[0m [torch]7m━\u001b[0m \u001b[32m23/24\u001b[0m [torch][nvidia-cusolver-cu12]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 cuda-bindings-12.9.4 cuda-pathfinder-1.3.4 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6.1 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.4.5 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.10.0 triton-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71254da1-3126-447b-a6bc-506e0108524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyaudio: filename=pyaudio-0.2.14-cp312-cp312-linux_x86_64.whl size=27687 sha256=3d7710efdf8accdc8cac77f4bd3a5f04c5ebff71f1e4f1dc72c8b4fb4e5a7ccb\n",
      "  Stored in directory: /home/hubenschmidt/.cache/pip/wheels/68/c7/33/c6a6b210cb5819ec5c219928c794a447742a7d86d21c0b92e6\n",
      "Successfully built pyaudio\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n"
     ]
    }
   ],
   "source": [
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec5a59-024e-4fee-9c1d-5ca6c013e9ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCUDA not available. Using CPU for LLM processing.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mmain_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mmain_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run the full ASR + LLM + TTS pipeline\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Step 1: ASR\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m transcribed_text = \u001b[43masr_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Step 2: LLM\u001b[39;00m\n\u001b[32m     67\u001b[39m response_text = llm_process(transcribed_text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36masr_process\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mListening for your query...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m     recognizer.adjust_for_ambient_noise(source)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     audio = \u001b[43mrecognizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     28\u001b[39m     text = recognizer.recognize_google(audio)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SpeechLM/lib/python3.12/site-packages/speech_recognition/__init__.py:461\u001b[39m, in \u001b[36mRecognizer.listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    459\u001b[39m result = \u001b[38;5;28mself\u001b[39m._listen(source, timeout, phrase_time_limit, snowboy_configuration, stream)\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SpeechLM/lib/python3.12/site-packages/speech_recognition/__init__.py:493\u001b[39m, in \u001b[36mRecognizer._listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time > timeout:\n\u001b[32m    491\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[33m\"\u001b[39m\u001b[33mlistening timed out while waiting for phrase to start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m buffer = \u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[32m    495\u001b[39m frames.append(buffer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SpeechLM/lib/python3.12/site-packages/speech_recognition/__init__.py:192\u001b[39m, in \u001b[36mMicrophone.MicrophoneStream.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpyaudio_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/SpeechLM/lib/python3.12/site-packages/pyaudio/__init__.py:570\u001b[39m, in \u001b[36mPyAudio.Stream.read\u001b[39m\u001b[34m(self, num_frames, exception_on_overflow)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_input:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot input stream\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    569\u001b[39m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import speech_recognition as sr\n",
    "from transformers import pipeline\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import pyaudio\n",
    "from playsound import playsound\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.hip)\n",
    "# Initialize components\n",
    "recognizer = sr.Recognizer()\n",
    "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    "llm = pipeline(\"text-generation\", model=\"gpt2\")  # Simple LLM for demo\n",
    "output_file = \"response.mp3\"\n",
    "\n",
    "\n",
    "#### Step 1: Automatic Speech Recognition (ASR)\n",
    "#Defining function for ASR:\n",
    "\n",
    "def asr_process():\n",
    "    \"\"\"Step 1: Automatic Speech Recognition (ASR)\"\"\"\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for your query...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(f\"Transcribed: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio.\")\n",
    "        return None\n",
    "    except sr.RequestError:\n",
    "        print(\"ASR service error.\")\n",
    "        return None\n",
    "\n",
    "#### Step 2: Large Language Model (LLM)\n",
    "#Defining function for LLM:\n",
    "def llm_process(text):\n",
    "    \"\"\"Step 2: Large Language Model (LLM)\"\"\"\n",
    "    if text:\n",
    "        prompt = f\"User said: {text}. Respond appropriately.\"\n",
    "        response = llm(prompt, max_length=50, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        response_text = response.split(\"\\n\")[0]  # Extract clean response\n",
    "        print(f\"LLM Response: {response_text}\")\n",
    "        return response_text\n",
    "    return \"Sorry, I didn't understand.\"\n",
    "\n",
    "#### Step 3: Text-to-Speech (TTS)\n",
    "#Defining function for TTS:\n",
    "def tts_process(text):\n",
    "    \"\"\"Step 3: Text-to-Speech (TTS)\"\"\"\n",
    "    tts = gTTS(text=text, lang=\"en\")\n",
    "    tts.save(output_file)\n",
    "    print(\"Playing response...\")\n",
    "    playsound(output_file)\n",
    "    os.remove(output_file)  # Clean up\n",
    "\n",
    "#### Step 4: Run the full ASR + LLM + TTS pipeline\n",
    "#Defining function for full pipeline:\n",
    "def main_pipeline():\n",
    "    \"\"\"Run the full ASR + LLM + TTS pipeline\"\"\"\n",
    "    # Step 1: ASR\n",
    "    transcribed_text = asr_process()\n",
    "    # Step 2: LLM\n",
    "    response_text = llm_process(transcribed_text)\n",
    "    # Step 3: TTS\n",
    "    tts_process(response_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using ROCm for LLM processing.\")\n",
    "    else:\n",
    "        print(\"ROCm not available. Using CPU for LLM processing.\")\n",
    "    main_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55d775-cf3c-4f45-a3bc-fd4bf137c236",
   "metadata": {},
   "source": [
    "### Extension of Previous Example - Using superior gpt-4o-mini model instead of GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7169c4-14ac-430b-aa1b-4322d699e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a8dfdf-5687-4ce7-b1a0-466cd9f1909f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for your query...\n",
      "Transcribed: how are you there\n",
      "LLM Response: I'm doing well, thank you! How about you?\n",
      "Playing response...\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import openai\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import pyaudio\n",
    "from playsound import playsound\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize components\n",
    "recognizer = sr.Recognizer()\n",
    "output_file = \"response.mp3\"\n",
    "\n",
    "def asr_process():\n",
    "    \"\"\"Step 1: Automatic Speech Recognition (ASR)\"\"\"\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening for your query...\")\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(f\"Transcribed: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio.\")\n",
    "        return None\n",
    "    except sr.RequestError:\n",
    "        print(\"ASR service error.\")\n",
    "        return None\n",
    "\n",
    "def llm_process(text):\n",
    "    \"\"\"Step 2: Large Language Model (LLM) using OpenAI GPT-4o-mini\"\"\"\n",
    "    if text:\n",
    "        prompt = f\"User said: {text}. Respond appropriately.\"\n",
    "        try:\n",
    "            client = openai.OpenAI(api_key=openai_api_key)\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=50,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            print(f\"LLM Response: {response_text}\")\n",
    "            return response_text\n",
    "        except Exception as e:\n",
    "            print(f\"LLM API error: {e}\")\n",
    "            return \"Sorry, I couldn't process your request.\"\n",
    "    return \"Sorry, I didn't understand.\"\n",
    "\n",
    "def tts_process(text):\n",
    "    \"\"\"Step 3: Text-to-Speech (TTS)\"\"\"\n",
    "    tts = gTTS(text=text, lang=\"en\")\n",
    "    tts.save(output_file)\n",
    "    print(\"Playing response...\")\n",
    "    playsound(output_file)\n",
    "    os.remove(output_file)  # Clean up\n",
    "\n",
    "def main_pipeline():\n",
    "    \"\"\"Run the full ASR + LLM + TTS pipeline\"\"\"\n",
    "    # Step 1: ASR\n",
    "    transcribed_text = asr_process()\n",
    "    # Step 2: LLM\n",
    "    response_text = llm_process(transcribed_text)\n",
    "    # Step 3: TTS\n",
    "    tts_process(response_text)\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "        main_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa851b8-fd19-4f70-bb44-8986a8938d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SpeechLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
